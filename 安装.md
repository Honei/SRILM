[toc]

# SRILM 
## 1. 介绍

`SRILM` 是一个应用比较广泛的统计和分析语言模型的工具，另外，还有 `Kenlm`，`IRSTLM`，`MITLM` 等语言模型工具。本文就 `SRILM` 的安装和使用做简单的介绍。

`SRILM` 的主要目标是支持语言模型的估计和评测。估计是从训练数据（训练集）中得到一个模型，包括最大似然估计及相应的平滑算法；而评测则是从测试集中计算其困惑度。其最基础和最核心的模块是 `n-gram` 模块，这也是最早实现的模块，包括两个工具：`ngram-count` 和 `ngram`，相应的被用来估计语言模型和计算语言模型的困惑度。
## 2. 安装

### 2.1 下载：
  http://www.speech.sri.com/projects/srilm/download.html
  在此网站上下载，需要填写个人信息后，点击accept进行下载。

### 2.2 解压：
  `tar zxvf srilm-1.7.2.tar.gz`

### 2.3 准备：
  在安装SRILM之前首先通过tclsh命令检查是否安装tcl工具（tcl脚本解释工具）未安装的情况，需要下载编译： 
  http://www.tcl.tk/software/tcltk/download.html（下载地址）
  ```
    tar zxvf tcl8.6.8-src.tar.gz
    ./configure
    make
    make install
  ```

### 2.4 安装：
在解压后的根目录下有INSTALL文件，其内容为安装说明，在此不再做内容解释，根据其所述，进行编译安装。
修改根目录下的Makefile：
第七行：`#SRILM = /home/speech/stolcke/project/srilm/devel修改为SRILM = $(PWD)  `
第十三行：`include $(SRILM)/common/Makefile.common.variables修改为include $(SRILM)/common/Makefile.machine.i686-m64`

此处需要根据自己的设备进行相应的配置，在 `common` 文件夹下有非常多的相应设备系统文件，可以使用 `uname -a` 命令来查看自己的设备信息，在 `sbin/machine_type`文件可以看到具体选择脚本，我的是X86_64,因此可以选用 `Makefile.machine.i686-m64`。

### 2.5 编译：
```
make World
```
在编译的过程中，根据自己电脑的软件安装情况可能会出现依赖软件未安装的情况，根据提示进行安装就可以继续执行了。

环境变量：
```
export PATH=/your/srilm/path/bin/:/your/srilm/path/bin:$PATH
```
测试：
```
make test
```
执行完成后，可以在./utils/test/output/下查看执行结果。下面我们就可以使用SRILM来建立语言模型了。

## 3 使用：
由于计算机内存的限制，将处理的文件分为小文件和大文件两种：

### 3.1 小文件：
词频统计：
```
ngram-count -text train.text -order 3 -write 1-train.count
```
模型训练：
```
ngram-count -read train.txt.count -order 3 -lm train.lm -interpolate -kndiscount
ngram-count -read train.txt.count -order 3 -lm train.lm -interpolate -kndiscount
```
* -read指向输入文件，为上一步的输出文件

* -order与上同

* -lm指向训练好的语言模型输出文件

最后两个参数为所采用的平滑方法，-interpolate为插值平滑，-kndiscount为 modified　Kneser-Ney 打折法，这两个是联合使用的

计算困惑度：
```
ngram -ppl test.txt -order 3 -lm LM >　result
```
* -ppl为对测试集句子进行评分(logP(T)，其中P(T)为所有句子的概率乘积）和计算测试集困惑度的参数

result为输出结果文件

其他参数同上。

大文件：
切分：
```
split -l line_num_of_file inputfile outputfiledir /outputfie_prefix
```
例：split -l 10000 train.txt filedir/

具体split的使用方法可以自己查询。

单个文件词频统计：
```
make-batch-counts  file-list 1 cat counts -order 3 -sort
```
file-list是个文件名，这个文件存储了你分割大文件而得到的小文件的文件名；5的意识是每5个小文件用于一次ngram-count训练，获得对应的count文件；cat 是用于过滤输出的脚本名，我们这里直接输出；后面的是传给ngram-count的参数，可以自己根据需要写。

合并结果：
```
 merge-batch-counts [ -l N ] counts [ filename-list ]
```
例：merge-batch-counts ./counts

将counts目录下的所有文件合并成一个文件，如果有些文件不用参与合并，可以在最后添加一个filename-list，只有在filename-list里面出现的文件才会被用于合并；-l N参数之处，一次同时合并N个文件。

训练语言模型：
```
make-big-lm -read ../counts/*.ngrams.gz -lm ../split.lm -order 3
```
用法同ngram-count

计算困惑度：
```
ngram -ppl filepath.txt -order 3 -lm split.lm -debug 2 > file.ppl
```
备注：
语料必须是分好的词，即用空格隔开，英文的书写都是用空格隔开的，但中文的书写各个字词之间并未空格，因此需要使用分词工具将文本自动分词，中文自动分词也是技术活，属于NLP范畴，常用的分词工具有：jieba、SnowNLP、pynlpir，thulac等，可以自己下载使用。

## 参考
1. https://www.52nlp.cn/language-model-training-tools-srilm-details
2. https://blog.csdn.net/atcmy/article/details/53780619?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&dist_request_id=1328769.325.16172705877614899&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control
3. https://ynuwm.github.io/2017/05/24/SRILM%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98/
4. https://blog.csdn.net/kevinfight/article/details/6423828